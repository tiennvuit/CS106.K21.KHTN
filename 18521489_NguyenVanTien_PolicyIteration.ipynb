{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18521489_NguyenVanTien_PolicyIteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBQFSJLClB+cD3pk3MbW/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiennvuit/CS106.K21.KHTN/blob/master/18521489_NguyenVanTien_PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIu4An4iS9z9",
        "colab_type": "text"
      },
      "source": [
        "# Cài đặt và đánh giá thuật toán Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Yo_qAVTIo6",
        "colab_type": "text"
      },
      "source": [
        "## Cài đặt thuật toán toán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9lmv0bpTHe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjZRNYSuTGec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Reference: https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
        "    total reward.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    policy: the policy to be used.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    returns:\n",
        "    total reward: real value of the total reward recieved by agent under policy.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
        "    \"\"\" Evaluates a policy by running it n times.\n",
        "    returns:\n",
        "    average total reward\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "            run_episode(env, policy, gamma = gamma, render = False)\n",
        "            for _ in range(n)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "\n",
        "def value_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Value-iteration algorithm \"\"\"\n",
        "    v = np.zeros(env.nS)  # initialize value-function\n",
        "    max_iterations = 100000\n",
        "    eps = 1e-20\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
        "            v[s] = max(q_sa)\n",
        "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
        "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
        "            break\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMwCv1AyXptn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e0513c4-9328-4f49-ef7a-97836c3e7645"
      },
      "source": [
        "env_name  = 'FrozenLake8x8-v0'\n",
        "gamma = 0.8\n",
        "env = gym.make(env_name)\n",
        "\n",
        "start_time = time.time()\n",
        "optimal_v = value_iteration(env, gamma)\n",
        "policy = extract_policy(optimal_v, gamma)\n",
        "print(\"Execute time: {}\".format(time.time()-start_time))\n",
        "\n",
        "policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
        "print('Policy average score = ', policy_score)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value-iteration converged at iteration# 2357.\n",
            "Execute time: 1.9779870510101318\n",
            "Policy average score =  7.802620389899087e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL02HmM6U2AD",
        "colab_type": "text"
      },
      "source": [
        "## Đánh giá thuật toán Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggeUBeBgacig",
        "colab_type": "text"
      },
      "source": [
        "- Thuật toán Policy Iteration sẽ tốt hơn thuật toán Value Iteration về mặt tính toán vì thường sẽ hội tụ đến điểm tối ưu nhanh hơn.\n",
        "- "
      ]
    }
  ]
}