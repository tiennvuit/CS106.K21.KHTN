{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18521489_NguyenVanTien_PolicyIteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjUBQiipOmeHxRmlyThvUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiennvuit/CS106.K21.KHTN/blob/master/18521489_NguyenVanTien_PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIu4An4iS9z9",
        "colab_type": "text"
      },
      "source": [
        "# Cài đặt và đánh giá thuật toán Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Yo_qAVTIo6",
        "colab_type": "text"
      },
      "source": [
        "## Cài đặt thuật toán toán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9lmv0bpTHe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjZRNYSuTGec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Reference: https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
        "    total reward.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    policy: the policy to be used.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    returns:\n",
        "    total reward: real value of the total reward recieved by agent under policy.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
        "    \"\"\" Evaluates a policy by running it n times.\n",
        "    returns:\n",
        "    average total reward\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "            run_episode(env, policy, gamma = gamma, render = False)\n",
        "            for _ in range(n)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "\n",
        "def value_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Value-iteration algorithm \"\"\"\n",
        "    v = np.zeros(env.nS)  # initialize value-function\n",
        "    max_iterations = 100000\n",
        "    eps = 1e-20\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
        "            v[s] = max(q_sa)\n",
        "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
        "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
        "            break\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMwCv1AyXptn",
        "colab_type": "code",
        "outputId": "f98fcbcb-2b42-4ffe-9de9-d8acbd537ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "env_name  = 'FrozenLake8x8-v0'\n",
        "gamma = 0.8\n",
        "env = gym.make(env_name)\n",
        "\n",
        "start_time = time.time()\n",
        "optimal_v = value_iteration(env, gamma)\n",
        "policy = extract_policy(optimal_v, gamma)\n",
        "print(\"Execute time: {}\".format(time.time()-start_time))\n",
        "\n",
        "policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
        "print('Policy average score = ', policy_score)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value-iteration converged at iteration# 2357.\n",
            "Execute time: 1.8212459087371826\n",
            "Policy average score =  5.146928376457513e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL02HmM6U2AD",
        "colab_type": "text"
      },
      "source": [
        "## Đánh giá thuật toán Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlED3niZszcj",
        "colab_type": "text"
      },
      "source": [
        "[Đây](https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration) là cuộc bàn luận về câu hỏi phân biệt giữa Value Iteration và Policy Iteration trên diễn đàng stackoverflow, dựa vào cuộc bàn luận này và kiến thức học được em có một số so sánh về 2 thuật toán được trình bày như sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gA0UBiTtbF0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Thuật toán Policy Iteration gồm có hai giai đoạn được lặp lại: policy evaluation và policy improvement; việc lặp lại này dừng khi chiến lược tìm được hội tụ. Trong khi đó, thuật toán Value Iteration gồm 2 giai đoạn được lặp lại là **finding optimal value function** và **policy extraction** và không hề có việc lặp lại vì một khi thuật toán đạt được hàm tối ưu thì chiến thuật được suy ra từ nó sẽ cũng tối ưu.\n",
        "\n",
        "*   Phân tích thêm một chút về giai đoạn **finding optimal value function** thì ta có thể nhận thấy nó chính là việc kết hợp của *policy improvement* và một phần *policy evaluation*.\n",
        "\n",
        "\n",
        "*Về mặt tốc độ thì thuật toán Policy Iteration thường sẽ nhanh hơn thuật toán Value Iteration vì một chiến lược hội tụ nhanh hơn hàm giá trị.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo8CNhZ0wuZZ",
        "colab_type": "text"
      },
      "source": [
        "![The comparision between two algorithms](https://i.stack.imgur.com/wGuj5.png)"
      ]
    }
  ]
}