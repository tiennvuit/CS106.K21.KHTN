{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18521489_NguyenVanTien_ValueIteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOe7V4U16gX2l3tiDJyBTEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiennvuit/CS106.K21.KHTN/blob/master/18521489_NguyenVanTien_ValueIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwnwzywAuISO",
        "colab_type": "text"
      },
      "source": [
        "# Đánh giá thuật toán Value Iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIBuitEquRLF",
        "colab_type": "text"
      },
      "source": [
        "## Cài đặt thuật toán dựa trên source có sẵn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54bzQC5VmHMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HymkaVR3myEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ivUq_NzgIs3bXHN4mn6zfhg2VXs2uNtv\n",
        "\"\"\"\n",
        "\n",
        "class PlayGame():\n",
        "    def __init__(self, name: str, max_iters=3, gamma=0.9):\n",
        "        self.env = gym.make(name)\n",
        "        self.max_iters = max_iters\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def value_iteration(self):\n",
        "        v_values = np.zeros(self.env.observation_space.n)\n",
        "        for i in range(self.max_iters):\n",
        "            prev_v_values = np.copy(v_values)\n",
        "\n",
        "            # Compute value for each state\n",
        "            for state in range(self.env.observation_space.n):\n",
        "                q_values = []\n",
        "\n",
        "                # Compute q-value for each action\n",
        "                for action in range(self.env.action_space.n):                \n",
        "                    q_value = 0\n",
        "                    for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                        q_value += prob * (reward + self.gamma * prev_v_values[next_state])\n",
        "                    q_values.append(q_value)\n",
        "                \n",
        "                # Select the best action\n",
        "                best_action = np.argmax(np.asarray(q_values))\n",
        "                v_values[state] = q_values[best_action]\n",
        "            \n",
        "            # Check convergence\n",
        "            if np.all(np.isclose(v_values, prev_v_values)):\n",
        "                print('Converged at {}-th iteration.'.format(i))\n",
        "                break\n",
        "        \n",
        "        return v_values\n",
        "\n",
        "    def policy_extraction(self):\n",
        "        v_values = self.value_iteration()\n",
        "        policy = np.zeros(self.env.observation_space.n, dtype=np.int)\n",
        "        \n",
        "        # Compute the best action for each state in the game\n",
        "        # Compute q-values for each (state-action) pair in the game\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Compute q-values for each action\n",
        "            for action in range(self.env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                    q_value += prob * (reward + self.gamma * v_values[next_state])\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # Select the best action\n",
        "            best_action = np.argmax(np.asarray(q_values))\n",
        "            policy[state] = best_action\n",
        "        \n",
        "        return policy\n",
        "\n",
        "\n",
        "    def play(self):\n",
        "        policy = self.policy_extraction()\n",
        "        state = self.env.reset()\n",
        "        steps = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            steps += 1\n",
        "            state = next_state\n",
        "\n",
        "        return (reward, steps)\n",
        "\n",
        "    def play_multiple_times(self):\n",
        "        num_episodes = 1000\n",
        "        list_of_steps = []\n",
        "        num_failures = 0\n",
        "        \n",
        "        for i in range(num_episodes):\n",
        "            reward, steps = self.play()\n",
        "            if reward == 1:\n",
        "                list_of_steps.append(steps)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "\n",
        "        print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "        print('avg. # steps: {}'.format(np.mean(list_of_steps)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_LqdtPnxxW",
        "colab_type": "code",
        "outputId": "f7ba319b-1f9e-47e4-bb67-f5752efe0a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "FrozenLake_v0 = PlayGame(\"FrozenLake-v0\", max_iters=20, gamma=0.8)\n",
        "FrozenLake_v0.play_multiple_times()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 538/1000\n",
            "avg. # steps: 29.227272727272727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uXMj2gpOWq",
        "colab_type": "code",
        "outputId": "a79d64e2-8b00-4db9-e1e3-482a217fdf48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "FrozenLake8x8_v0 = PlayGame(\"FrozenLake8x8-v0\", max_iters=20, gamma=0.8)\n",
        "FrozenLake8x8_v0.play_multiple_times()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 377/1000\n",
            "avg. # steps: 72.58587479935794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGARJUKqnNX",
        "colab_type": "code",
        "outputId": "348f3179-5469-4607-8a94-5c82fc90d559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "Taxi_v3 = PlayGame(\"FrozenLake8x8-v0\", max_iters=20, gamma=0.8)\n",
        "Taxi_v3.play_multiple_times()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 373/1000\n",
            "avg. # steps: 70.17543859649123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K513FWVit4Xc",
        "colab_type": "text"
      },
      "source": [
        "## Một số nhận sét về thuật toán Value Iteration:\n",
        "- Ưu điểm:\n",
        "    - Độ phức tạp của mỗi vòng lặp là $O(S^2A)$.\n",
        "    - Sẽ hội tụ đến điểm tối ưu.\n",
        "    - Value iteration is good for a small set of states because we will avoid computing very deep expectimax trees which run in exponential time.\n",
        "    - Thuật toán này phù hợp cho những bài toán không gian trạng thái nhỏ vì chúng ta sẽ tránh được việc tính toán lớn đi duyệt sâu xuống câu expectimax - thời gian thực thi là hàm mũ.\n",
        "\n",
        "- Nhược điểm:\n",
        "    - Thuật toán này phải duyệt qua mọi trạng thái trong mỗi lần lặp và vì vậy nếu chúng ta có không gian trạng thái lớn  việc lặp lại giá trị phải tiêu tốn rất nhiều thời gian.\n",
        "    - The \"max\" at a state rarely changes. This means that the relative size of the Q values converge well before the values converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo0G6hTg0TDi",
        "colab_type": "text"
      },
      "source": [
        "## Tham khảo:\n",
        "- [Value Iteration](https://cs188ai.fandom.com/wiki/Value_Iteration)\n"
      ]
    }
  ]
}