{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18521489_NguyenVanTien_ValueIteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiennvuit/CS106.K21.KHTN/blob/master/18521489_NguyenVanTien_ValueIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwnwzywAuISO",
        "colab_type": "text"
      },
      "source": [
        "# Đánh giá thuật toán Value Iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIBuitEquRLF",
        "colab_type": "text"
      },
      "source": [
        "## Cài đặt thuật toán dựa trên source có sẵn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54bzQC5VmHMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HymkaVR3myEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlayGame():\n",
        "    def __init__(self, name: str, max_iters=3, gamma=0.9):\n",
        "        self.env = gym.make(name)\n",
        "        self.max_iters = max_iters\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def value_iteration(self):\n",
        "        v_values = np.zeros(self.env.observation_space.n)\n",
        "        for i in range(self.max_iters):\n",
        "            prev_v_values = np.copy(v_values)\n",
        "\n",
        "            # Compute value for each state\n",
        "            for state in range(self.env.observation_space.n):\n",
        "                q_values = []\n",
        "\n",
        "                # Compute q-value for each action\n",
        "                for action in range(self.env.action_space.n):                \n",
        "                    q_value = 0\n",
        "                    for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                        q_value += prob * (reward + self.gamma * prev_v_values[next_state])\n",
        "                    q_values.append(q_value)\n",
        "                \n",
        "                # Select the best action\n",
        "                best_action = np.argmax(np.asarray(q_values))\n",
        "                v_values[state] = q_values[best_action]\n",
        "            \n",
        "            # Check convergence\n",
        "            if np.all(np.isclose(v_values, prev_v_values)):\n",
        "                print('Converged at {}-th iteration.'.format(i))\n",
        "                break\n",
        "        \n",
        "        return v_values\n",
        "\n",
        "    def policy_extraction(self):\n",
        "        v_values = self.value_iteration()\n",
        "        policy = np.zeros(self.env.observation_space.n, dtype=np.int)\n",
        "        \n",
        "        # Compute the best action for each state in the game\n",
        "        # Compute q-values for each (state-action) pair in the game\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Compute q-values for each action\n",
        "            for action in range(self.env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                    q_value += prob * (reward + self.gamma * v_values[next_state])\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # Select the best action\n",
        "            best_action = np.argmax(np.asarray(q_values))\n",
        "            policy[state] = best_action\n",
        "        \n",
        "        return policy\n",
        "\n",
        "\n",
        "    def play(self):\n",
        "        policy = self.policy_extraction()\n",
        "        state = self.env.reset()\n",
        "        steps = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            steps += 1\n",
        "            state = next_state\n",
        "\n",
        "        return (reward, steps)\n",
        "\n",
        "    def play_multiple_times(self):\n",
        "        num_episodes = 1000\n",
        "        list_of_steps = []\n",
        "        num_failures = 0\n",
        "        \n",
        "        for i in range(num_episodes):\n",
        "            reward, steps = self.play()\n",
        "            if reward > 0:\n",
        "                list_of_steps.append(steps)\n",
        "            else:\n",
        "                num_failures += 1\n",
        "\n",
        "        print('# failures: {}/{}'.format(num_failures, num_episodes))\n",
        "        print('avg. # steps: {}'.format(np.mean(list_of_steps)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_LqdtPnxxW",
        "colab_type": "code",
        "outputId": "05d37f02-7701-4bc1-efa5-a36f42eefe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "FrozenLake_v0 = PlayGame(\"FrozenLake-v0\", max_iters=30, gamma=0.8)\n",
        "FrozenLake_v0.play_multiple_times()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 578/1000\n",
            "avg. # steps: 28.509478672985782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uXMj2gpOWq",
        "colab_type": "code",
        "outputId": "08f38160-3748-4b6c-a9eb-2f3519779c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "FrozenLake8x8_v0 = PlayGame(\"FrozenLake8x8-v0\", max_iters=30, gamma=0.8)\n",
        "FrozenLake8x8_v0.play_multiple_times()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 373/1000\n",
            "avg. # steps: 70.67783094098884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGARJUKqnNX",
        "colab_type": "code",
        "outputId": "ab3f758f-8685-4b4c-c6a4-189451e52ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Taxi_v3 = PlayGame(\"Taxi-v3\", max_iters=30, gamma=0.8)\n",
        "Taxi_v3.play_multiple_times()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# failures: 0/1000\n",
            "avg. # steps: 13.075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K513FWVit4Xc",
        "colab_type": "text"
      },
      "source": [
        "## Một số nhận sét về thuật toán Value Iteration:\n",
        "- Ưu điểm:\n",
        "    - Độ phức tạp của mỗi vòng lặp là $O(S^2A)$, với $S$ là độ lớn của không gian trạng thái, $A$ là số lượng hành động lớn nhất có thể có của một trạng thái và đảm bảo việc hội tụ.\n",
        "\n",
        "    - Thuật toán này phù hợp cho những bài toán không gian trạng thái nhỏ vì chúng ta sẽ tránh được việc tính toán lớn đi duyệt sâu xuống câu expectimax - thời gian thực thi là hàm mũ.\n",
        "\n",
        "- Nhược điểm:\n",
        "    - Thuật toán này phải duyệt qua mọi trạng thái trong mỗi lần lặp và vì vậy nếu chúng ta có không gian trạng thái lớn  việc lặp lại giá trị phải tiêu tốn rất nhiều thời gian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo0G6hTg0TDi",
        "colab_type": "text"
      },
      "source": [
        "## Tham khảo:\n",
        "- [Value Iteration](https://cs188ai.fandom.com/wiki/Value_Iteration)\n"
      ]
    }
  ]
}